{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nuaeq3NnSU8_",
    "outputId": "928797ea-8fbc-4e2d-c880-5cae7f048caa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./opt/anaconda3/lib/python3.9/site-packages (23.1.1)\n",
      "Requirement already satisfied: emot in ./opt/anaconda3/lib/python3.9/site-packages (3.1)\n",
      "Requirement already satisfied: emoji in ./opt/anaconda3/lib/python3.9/site-packages (2.2.0)\n",
      "Requirement already satisfied: hazm in ./opt/anaconda3/lib/python3.9/site-packages (0.7.0)\n",
      "Requirement already satisfied: nltk==3.3 in ./opt/anaconda3/lib/python3.9/site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: libwapiti>=0.2.1 in ./opt/anaconda3/lib/python3.9/site-packages (from hazm) (0.2.1)\n",
      "Requirement already satisfied: six in ./opt/anaconda3/lib/python3.9/site-packages (from nltk==3.3->hazm) (1.16.0)\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/akzanuali/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade pip  \n",
    "!pip install emot\n",
    "!pip install emoji\n",
    "!pip install hazm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import preprocessor as prepr\n",
    "import re\n",
    "import ast\n",
    "import hazm\n",
    "import nltk\n",
    "\n",
    "from tqdm.notebook import tqdm  \n",
    "from multiprocessing import Pool\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5DH4-gQFTbAN"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"MirasIrony.csv\", names=['text', 'label'], index_col = None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" نماینده مجلس خطاب به قربانی اسیدپاشی : ازدس...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_آرره دیگه بعدش گفتم استاد دارید مسخره میکنید ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_چجوری میتونی اصفهانی باشی و هوادار پرسپولیس؟ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- داداش #سربازی ت کی تموم میشه ایشالا؟ + چیزی ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- در گذشته زندگی می کنی یا حال؟ اگر در گذشته ز...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0   \" نماینده مجلس خطاب به قربانی اسیدپاشی : ازدس...     0\n",
       "1  _آرره دیگه بعدش گفتم استاد دارید مسخره میکنید ...     1\n",
       "2  _چجوری میتونی اصفهانی باشی و هوادار پرسپولیس؟ ...     1\n",
       "3  - داداش #سربازی ت کی تموم میشه ایشالا؟ + چیزی ...     0\n",
       "4  - در گذشته زندگی می کنی یا حال؟ اگر در گذشته ز...     1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PERSIAN_EMOJIS.txt') as f:\n",
    "    d = f.read()\n",
    "PERSIAN_EMOJIS = ast.literal_eval(d)\n",
    "\n",
    "with open('PERSIAN_EMOTICONS.txt') as f:\n",
    "    d = f.read()\n",
    "PERSIAN_EMOTICONS = ast.literal_eval(d)\n",
    "\n",
    "stopwords = list(pd.read_csv(\"persian_stopwords.csv\", names=['word', '-'], sep = ';',index_col = None, header=None)[1:]['word'].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords + ['«','»','،',':','؟','.',',','\"','*'])\n",
    "stemmer = hazm.Stemmer()\n",
    "normalizer = hazm.Normalizer()\n",
    "\n",
    "prepr.set_options(prepr.OPT.URL, prepr.OPT.MENTION, prepr.OPT.HASHTAG)\n",
    "\n",
    "def convert_emoji_to_text(text):\n",
    "    for emot in PERSIAN_EMOJIS:\n",
    "        text = text.replace(emot, \"_\".join(PERSIAN_EMOJIS[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "def convert_emoticon_to_text(text):\n",
    "    for emot in PERSIAN_EMOTICONS:\n",
    "        text = text.replace(emot, \"_\".join(PERSIAN_EMOTICONS[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "        \n",
    "def clean_text(text):\n",
    "    text = str(text) \n",
    "    text = normalizer.normalize(text)\n",
    "    text = convert_emoji_to_text(text)                                               #converting emojis to text\n",
    "    text = convert_emoticon_to_text(text)  \n",
    "    text = prepr.clean(text)                 \n",
    "    text = [stemmer.stem(token) for token in hazm.word_tokenize(text)]      #stemmatize+tokenization\n",
    "    text = [item for item in text if item not in stop_words]                  #stopwords\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa2a2dad141495e8d35e7509c6cba13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = data.text\n",
    "cleaned_texts = []\n",
    "for item in tqdm(texts, total = len(texts)):\n",
    "    cleaned_texts.append(clean_text(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce5b0f5556e47fb830a28ef73734505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_texts = []\n",
    "for text in tqdm(cleaned_texts, total = len(cleaned_texts)):\n",
    "    final_texts.append(' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in cleaned_texts:\n",
    "    for word in text:\n",
    "        cnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(final_texts, data['label'].values, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics  \n",
    "def metrics_result(actual, predict):                       \n",
    "    print('Metrics:')\n",
    "    print ('Accuracy: {0: .3f}'.format(metrics.accuracy_score(actual, predict)) ) \n",
    "    print ('Precision: {0: .3f}'.format(metrics.precision_score(actual, predict,average='weighted')) ) \n",
    "    print ('Recall: {0: 0.3f}'.format(metrics.recall_score(actual, predict,average='weighted'))   )\n",
    "    print ('F1-score: {0:.3f}'.format(metrics.f1_score(actual, predict,average='weighted'))   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Random Forest:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.676\n",
      "Precision:  0.676\n",
      "Recall:  0.676\n",
      "F1-score: 0.650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TF Idf + Random Forest\n",
    "text_clf = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', RandomForestClassifier())\n",
    "                     ])\n",
    " \n",
    "text_clf.fit(X_train, y_train)\n",
    "print('TF Idf + Random Forest:')\n",
    "print()\n",
    "predicted = text_clf.predict(X_test)\n",
    "metrics_result(y_test, predicted)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Logistic Regression:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.650\n",
      "Precision:  0.640\n",
      "Recall:  0.650\n",
      "F1-score: 0.638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_logreg = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('logreg', LogisticRegression())\n",
    "                     ])\n",
    " \n",
    "text_logreg.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Logistic Regression:')\n",
    "print()\n",
    "predicted_l = text_logreg.predict(X_test)\n",
    "metrics_result(y_test,predicted_l)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Naive Bayes:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.658\n",
      "Precision:  0.649\n",
      "Recall:  0.658\n",
      "F1-score: 0.640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_nb = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('nb', naive_bayes.MultinomialNB())\n",
    "                     ])\n",
    " \n",
    "text_nb.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Naive Bayes:')\n",
    "print()\n",
    "predicted_nb = text_nb.predict(X_test)\n",
    "metrics_result(y_test,predicted_nb)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + SVM:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.635\n",
      "Precision:  0.632\n",
      "Recall:  0.635\n",
      "F1-score: 0.634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_svm = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('svm', svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto'))\n",
    "                     ])\n",
    " \n",
    "text_svm.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + SVM:')\n",
    "print()\n",
    "predicted_svm = text_svm.predict(X_test)\n",
    "metrics_result(y_test,predicted_svm)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Knn:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.595\n",
      "Precision:  0.586\n",
      "Recall:  0.595\n",
      "F1-score: 0.588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_knn = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('knn', KNeighborsClassifier(n_neighbors=7))\n",
    "                     ])\n",
    " \n",
    "text_knn.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Knn:')\n",
    "print()\n",
    "predicted_knn = text_knn.predict(X_test)\n",
    "metrics_result(y_test,predicted_knn)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "TF Idf + Knn + Grid Search:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.610\n",
      "Precision:  0.602\n",
      "Recall:  0.610\n",
      "F1-score: 0.604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_range = list(range(1, 10))\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "text_knn_gr = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('knn_gr', GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=False,verbose=1))\n",
    "                     ])\n",
    " \n",
    "text_knn_gr.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Knn + Grid Search:')\n",
    "print()\n",
    "predicted_knn_gr = text_knn_gr.predict(X_test)\n",
    "metrics_result(y_test,predicted_knn_gr)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Gradient Boosting:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.664\n",
      "Precision:  0.657\n",
      "Recall:  0.664\n",
      "F1-score: 0.642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_gb = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('gb', GradientBoostingClassifier())\n",
    "                     ])\n",
    " \n",
    "text_gb.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Gradient Boosting:')\n",
    "print()\n",
    "predicted_gb = text_gb.predict(X_test)\n",
    "metrics_result(y_test,predicted_gb)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + SGD:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.608\n",
      "Precision:  0.618\n",
      "Recall:  0.608\n",
      "F1-score: 0.611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_sgd = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('sgd', SGDClassifier())\n",
    "                     ])\n",
    " \n",
    "text_sgd.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + SGD:')\n",
    "print()\n",
    "predicted_sgd = text_sgd.predict(X_test)\n",
    "metrics_result(y_test,predicted_sgd)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Decision Tree:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.599\n",
      "Precision:  0.593\n",
      "Recall:  0.599\n",
      "F1-score: 0.595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_dt = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('dt', DecisionTreeClassifier())\n",
    "                     ])\n",
    " \n",
    "text_dt.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Decision Tree:')\n",
    "print()\n",
    "predicted_dt = text_dt.predict(X_test)\n",
    "metrics_result(y_test,predicted_dt)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Ada Boost:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.650\n",
      "Precision:  0.640\n",
      "Recall:  0.650\n",
      "F1-score: 0.632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_ab = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('ab', AdaBoostClassifier())\n",
    "                     ])\n",
    " \n",
    "text_ab.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Ada Boost:')\n",
    "print()\n",
    "predicted_ab = text_ab.predict(X_test)\n",
    "metrics_result(y_test,predicted_ab)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [sentence.split() for sentence in X_train]\n",
    "w2v_model = Word2Vec(sentences, window=5, min_count=5, workers=4)\n",
    "\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "X_train_w2v = np.array([vectorize(sentence) for sentence in X_train])\n",
    "X_test_w2v = np.array([vectorize(sentence) for sentence in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec + Random Forest:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.589\n",
      "Precision:  0.579\n",
      "Recall:  0.589\n",
      "F1-score: 0.581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    " \n",
    "clf.fit(X_train_w2v, y_train)\n",
    "print('Word2Vec + Random Forest:')\n",
    "print()\n",
    "predicted = clf.predict(X_test_w2v)\n",
    "metrics_result(y_test, predicted)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec + LogisticRegression:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.597\n",
      "Precision:  0.529\n",
      "Recall:  0.597\n",
      "F1-score: 0.452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    " \n",
    "lgr.fit(X_train_w2v, y_train)\n",
    "print('Word2Vec + LogisticRegression:')\n",
    "print()\n",
    "predicted = lgr.predict(X_test_w2v)\n",
    "metrics_result(y_test, predicted)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec + SVM:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.597\n",
      "Precision:  0.356\n",
      "Recall:  0.597\n",
      "F1-score: 0.446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmm = svm.SVC(C=2.0, kernel='linear', degree=1, gamma='auto')\n",
    " \n",
    "svmm.fit(X_train_w2v, y_train)\n",
    "print('Word2Vec + SVM:')\n",
    "print()\n",
    "predicted = svmm.predict(X_test_w2v)\n",
    "metrics_result(y_test, predicted)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec + SGD:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.597\n",
      "Precision:  0.356\n",
      "Recall:  0.597\n",
      "F1-score: 0.446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    " \n",
    "sgd.fit(X_train_w2v, y_train)\n",
    "print('Word2Vec + SGD:')\n",
    "print()\n",
    "predicted = sgd.predict(X_test_w2v)\n",
    "metrics_result(y_test, predicted)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
