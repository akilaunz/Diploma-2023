{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nuaeq3NnSU8_",
    "outputId": "928797ea-8fbc-4e2d-c880-5cae7f048caa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./opt/anaconda3/lib/python3.9/site-packages (23.1)\n",
      "Requirement already satisfied: tweet-preprocessor in ./opt/anaconda3/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: emot in ./opt/anaconda3/lib/python3.9/site-packages (3.1)\n",
      "Requirement already satisfied: textblob in ./opt/anaconda3/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in ./opt/anaconda3/lib/python3.9/site-packages (from textblob) (3.3)\n",
      "Requirement already satisfied: six in ./opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.16.0)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/akzanuali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/akzanuali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/akzanuali/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade pip  \n",
    "!pip install tweet-preprocessor\n",
    "!pip install emot\n",
    "!pip install textblob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import nltk\n",
    "import preprocessor as prepr\n",
    "import re\n",
    "import emot\n",
    "import textblob\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords     \n",
    "from multiprocessing import Pool\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO, EMOJI_UNICODE\n",
    "import nltk\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# !pip3 install xgboost\n",
    "# !pip install xgboost\n",
    "# import xgboost \n",
    "# !pip3 install catboost\n",
    "# import catboost \n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5DH4-gQFTbAN"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train-balanced-sarcasm.csv\")\n",
    "data = df[['label', 'comment', 'parent_comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/akzanuali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "stop_words = stopwords.words('english') + ['.',',','\"','*']\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "# prepr.set_options(prepr.OPT.URL, prepr.OPT.EMOJI, prepr.OPT.MENTION, prepr.OPT.HASHTAG)\n",
    "prepr.set_options(prepr.OPT.URL, prepr.OPT.MENTION, prepr.OPT.HASHTAG)\n",
    "\n",
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "    \n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS_EMO:\n",
    "        text = text.replace(emot, \"_\".join(EMOTICONS_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "        \n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "#     text = str(TextBlob(text).correct())                                      #correcting spelling\n",
    "#     text = emoji_pattern.sub(r'', text)                                     #removing emoji \n",
    "    text = prepr.clean(text)                                                  #removing links+mention+hashtags\n",
    "    text = text.strip().lower()                                               #lowercase\n",
    "    text = re.sub(r'<.*?>', '', text, flags=re.MULTILINE)                     #removing tags\n",
    "    text = convert_emojis(text)                                               #converting emojis to text\n",
    "    text = convert_emoticons(text)                \n",
    "    text = [(lemmatizer.lemmatize(w)) for w in  w_tokenizer.tokenize((text))] #lemmatize+tokenization\n",
    "    text = [item for item in text if item not in stop_words]                  #stopwords\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fbb44995f8413cb3e6be761e598aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1010826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = data.comment\n",
    "cleaned_texts = []\n",
    "for item in tqdm(texts, total = len(texts)):\n",
    "    cleaned_texts.append(clean_text(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0445b20cbb145aa8416db3a822625c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1010826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_texts = []\n",
    "for text in tqdm(cleaned_texts, total = len(cleaned_texts)):\n",
    "    final_texts.append(convert_emoticons(' '.join(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?', 118049),\n",
       " ('!', 97521),\n",
       " ('...', 79067),\n",
       " ('like', 56427),\n",
       " ('wa', 55805),\n",
       " ('yeah', 41086),\n",
       " ('get', 38664),\n",
       " ('people', 36412),\n",
       " ('would', 35187),\n",
       " ('one', 33950)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in cleaned_texts:\n",
    "    for word in text:\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(final_texts, data['label'].values, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics  \n",
    "def metrics_result(actual, predict):                       \n",
    "    print('Metrics:')\n",
    "    print ('Accuracy: {0: .3f}'.format(metrics.accuracy_score(actual, predict)) ) \n",
    "    print ('Precision: {0: .3f}'.format(metrics.precision_score(actual, predict,average='weighted')) ) \n",
    "    print ('Recall: {0: 0.3f}'.format(metrics.recall_score(actual, predict,average='weighted'))   )\n",
    "    print ('F1-score: {0:.3f}'.format(metrics.f1_score(actual, predict,average='weighted'))   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Random Forest:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.683\n",
      "Precision:  0.683\n",
      "Recall:  0.683\n",
      "F1-score: 0.682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TF Idf + Random Forest\n",
    "text_clf = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', RandomForestClassifier())\n",
    "                     ])\n",
    " \n",
    "text_clf.fit(X_train, y_train)\n",
    "print('TF Idf + Random Forest:')\n",
    "print()\n",
    "predicted = text_clf.predict(X_test)\n",
    "metrics_result(y_test, predicted)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Logistic Regression:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.680\n",
      "Precision:  0.681\n",
      "Recall:  0.680\n",
      "F1-score: 0.679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_logreg = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('logreg', LogisticRegression())\n",
    "                     ])\n",
    " \n",
    "text_logreg.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Logistic Regression:')\n",
    "print()\n",
    "predicted_l = text_logreg.predict(X_test)\n",
    "metrics_result(y_test,predicted_l)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Naive Bayes:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.658\n",
      "Precision:  0.658\n",
      "Recall:  0.658\n",
      "F1-score: 0.658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_nb = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('nb', naive_bayes.MultinomialNB())\n",
    "                     ])\n",
    " \n",
    "text_nb.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Naive Bayes:')\n",
    "print()\n",
    "predicted_nb = text_nb.predict(X_test)\n",
    "metrics_result(y_test,predicted_nb)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + SVM:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.678\n",
      "Precision:  0.680\n",
      "Recall:  0.678\n",
      "F1-score: 0.678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_svm = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('svm', svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto'))\n",
    "                     ])\n",
    " \n",
    "text_svm.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + SVM:')\n",
    "print()\n",
    "predicted_svm = text_svm.predict(X_test)\n",
    "metrics_result(y_test,predicted_svm)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Knn:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.586\n",
      "Precision:  0.610\n",
      "Recall:  0.586\n",
      "F1-score: 0.561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_knn = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('knn', KNeighborsClassifier(n_neighbors=7))\n",
    "                     ])\n",
    " \n",
    "text_knn.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Knn:')\n",
    "print()\n",
    "predicted_knn = text_knn.predict(X_test)\n",
    "metrics_result(y_test,predicted_knn)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = list(range(1, 10))\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "text_knn_gr = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('knn_gr', GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=False,verbose=1))\n",
    "                     ])\n",
    " \n",
    "text_knn_gr.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Knn + Grid Search:')\n",
    "print()\n",
    "predicted_knn_gr = text_knn_gr.predict(X_test)\n",
    "metrics_result(y_test,predicted_knn_gr)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Gradient Boosting:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.616\n",
      "Precision:  0.657\n",
      "Recall:  0.616\n",
      "F1-score: 0.589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_gb = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('gb', GradientBoostingClassifier())\n",
    "                     ])\n",
    " \n",
    "text_gb.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Gradient Boosting:')\n",
    "print()\n",
    "predicted_gb = text_gb.predict(X_test)\n",
    "metrics_result(y_test,predicted_gb)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + SGD:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.653\n",
      "Precision:  0.675\n",
      "Recall:  0.653\n",
      "F1-score: 0.642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_sgd = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('sgd', SGDClassifier())\n",
    "                     ])\n",
    " \n",
    "text_sgd.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + SGD:')\n",
    "print()\n",
    "predicted_sgd = text_sgd.predict(X_test)\n",
    "metrics_result(y_test,predicted_sgd)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Decision Tree:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.633\n",
      "Precision:  0.633\n",
      "Recall:  0.633\n",
      "F1-score: 0.633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_dt = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('dt', DecisionTreeClassifier())\n",
    "                     ])\n",
    " \n",
    "text_dt.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Decision Tree:')\n",
    "print()\n",
    "predicted_dt = text_dt.predict(X_test)\n",
    "metrics_result(y_test,predicted_dt)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Idf + Ada Boost:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.613\n",
      "Precision:  0.641\n",
      "Recall:  0.613\n",
      "F1-score: 0.593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_ab = Pipeline([\n",
    "                     ('tfidf', TfidfVectorizer()),\n",
    "                     ('ab', AdaBoostClassifier())\n",
    "                     ])\n",
    " \n",
    "text_ab.fit(X_train, y_train)\n",
    "\n",
    "print('TF Idf + Ada Boost:')\n",
    "print()\n",
    "predicted_ab = text_ab.predict(X_test)\n",
    "metrics_result(y_test,predicted_ab)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [sentence.split() for sentence in X_train]\n",
    "w2v_model = Word2Vec(sentences, window=5, min_count=5, workers=4)\n",
    "\n",
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(100)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "X_train_w2v = np.array([vectorize(sentence) for sentence in X_train])\n",
    "X_test_w2v = np.array([vectorize(sentence) for sentence in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec + Random Forest:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.673\n",
      "Precision:  0.673\n",
      "Recall:  0.673\n",
      "F1-score: 0.673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    " \n",
    "clf.fit(X_train_w2v, y_train)\n",
    "print('Word2Vec + Random Forest:')\n",
    "print()\n",
    "predicted = clf.predict(X_test_w2v)\n",
    "metrics_result(y_test, predicted)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec + LogisticRegression:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.646\n",
      "Precision:  0.647\n",
      "Recall:  0.646\n",
      "F1-score: 0.646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    " \n",
    "lgr.fit(X_train_w2v, y_train)\n",
    "print('Word2Vec + LogisticRegression:')\n",
    "print()\n",
    "predicted = lgr.predict(X_test_w2v)\n",
    "metrics_result(y_test, predicted)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmm = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    " \n",
    "svmm.fit(X_train_w2v, y_train)\n",
    "print('Word2Vec + SVM:')\n",
    "print()\n",
    "predicted = svmm.predict(X_test_w2v)\n",
    "metrics_result(y_test, predicted)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec + SGD:\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.544\n",
      "Precision:  0.579\n",
      "Recall:  0.544\n",
      "F1-score: 0.490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    " \n",
    "sgd.fit(X_train_w2v, y_train)\n",
    "print('Word2Vec + SGD:')\n",
    "print()\n",
    "predicted = sgd.predict(X_test_w2v)\n",
    "metrics_result(y_test, predicted)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
